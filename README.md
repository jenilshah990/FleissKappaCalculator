# Fleiss Kappa Score & Visualisation of Video Annotations among Labellers

Fleiss' kappa (named after Joseph L. Fleiss) is a statistical measure for assessing the reliability of agreement between a fixed number of raters when assigning categorical ratings to a number of items or classifying items. Fleiss' kappa works for any number of raters giving categorical ratings, to a fixed number of items. It can be interpreted as expressing the extent to which the observed amount of agreement among raters exceeds what would be expected if all raters made their ratings completely randomly.

This tool was created by Jenil Shah for usage in DEVIATE Research @UMTRI to calculate the inter-rater reliability for annotated videos due to absence of any existing tool. The tool does the following:
1.) Create a Visualisation for the Labelled Matrix to get a clearer understanding of the labellings. 
2.) Converts a labelled video matrix into a Fleiss Matrix 
3.) Calculate the Overall Fleiss Kappa Score & Percent Overall Agreement among raters above chance.



