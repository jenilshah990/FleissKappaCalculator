# Fleiss Kappa Score & Visualisation of Video Annotations among Labellers

Fleiss' kappa (named after Joseph L. Fleiss) is a statistical measure for assessing the reliability of agreement between a fixed number of raters when assigning categorical ratings to a number of items or classifying items.

This tool was created by Jenil Shah for usage in DEVIATE Research @UMTRI to calculate the inter-rater reliability for annotated videos due to absence of any exusting tool. The tool converts a labelled video matrix into a Fleiss Matrix & calculate the Overall Fleiss Kappa Score & Percent Overall Agreement among raters above chance.


